{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tng00/Machine_Learning_MAI_2023/blob/main/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5372eb6b",
      "metadata": {
        "id": "5372eb6b"
      },
      "source": [
        "# Домашняя работа по регуляризации и оптимизации\n",
        "\n",
        "Ниже приводится корпус данных с двумя метками: 1 и -1. К данным применяется линейная модель классификации:\n",
        "\n",
        "$f(x, \\theta) = x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3.$\n",
        "\n",
        "Предлагается подобрать параметры $\\theta$ минимизируя следующую функцию ошибки:\n",
        "\n",
        "$\\mathcal{L}(\\theta) = 0.1 \\|\\theta\\|^2 + \\frac{1}{N}\\sum\\limits_{i=1}^N \\max(0, 1 - y_i f(x_i, \\theta)).$\n",
        "\n",
        "Для оптимизации предлагается использовать метод градиентного спуска с 1000 шагами размера $0.1$ из начальной точки $(1, 1, 0)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84be44bf",
      "metadata": {
        "id": "84be44bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8e381337",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e381337",
        "outputId": "aae88112-ece7-4f82-b1a7-1609d9ddf8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [ 1.   2.   1.   0.  -0.5]\n",
            "Loss: 0.5\n"
          ]
        }
      ],
      "source": [
        "X = np.array([\n",
        "    [0, 1],\n",
        "    [1, 1],\n",
        "    [1, 0],\n",
        "    [-0.5, 0.5],\n",
        "    [0, -0.5]\n",
        "])\n",
        "\n",
        "y = np.array([1, 1, 1, -1, -1])\n",
        "\n",
        "theta0 = np.array([1.0, 1.0, 0.0])\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "def f(X, theta):\n",
        "    theta = np.asarray(theta)\n",
        "    return (X * theta[:2]).sum(axis=-1) + theta[2]\n",
        "\n",
        "def loss(X, y, theta):\n",
        "    theta = np.asarray(theta)\n",
        "    norm = (theta ** 2).sum()\n",
        "    deltas = y * f(X, theta)\n",
        "    return 0.1 * norm + np.mean(np.maximum(0, 1 - deltas))\n",
        "\n",
        "print(\"Prediction:\", f(X, theta0))\n",
        "print(\"Loss:\", loss(X, y, theta0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Градиент функции потерь\n",
        "$$\\mathfrak{L}(\\theta) = 0.1 ||\\theta||^2 + \\frac{1}{N} \\sum^N_{i=1} \\max(0, 1-y_i f(x_i, \\theta))  $$\n",
        "\n",
        "$$f(x, \\theta) = x_1 \\theta_1 + x_2 \\theta_2 + \\theta_3 $$\n",
        "\n",
        "Будем считать градиент относительно $θ$. \n",
        "\n",
        "Далее, верхним индексом будем обозначать столбцы, нижним - строки.\n",
        "\n",
        "Пусть $ϕ(\\theta_i, j) = max(0, 1 - y_j \\cdot φ_j(\\theta)), \\ \\ φ_j(\\theta) = f(x_j, \\theta), \\ \\ i \\in \\{1..3\\}, \\ \\ j \\in \\{1..N\\}$\n",
        "\n",
        "Для начала, посчитаем производные $φ_j(\\theta)$: \\\\\n",
        "$$\\frac{∂φ_j}{∂θ_1} = x^1_j$$\n",
        "$$\\frac{∂φ_j}{∂θ_2} = x^2_j$$\n",
        "$$\\frac{∂φ_j}{∂θ_3} = 1$$\n",
        "$$\\nabla φ_j = \\begin{pmatrix} x^1_j \\\\ x^2_j \\\\ 1 \\end{pmatrix}$$\n",
        "\n",
        "По правилу дифференцирования сложной функции:\n",
        "$$\\frac{∂ϕ}{∂θ} = \\frac{∂\\phi}{∂φ}\\frac{∂φ}{∂θ}$$\n",
        "\n",
        "Отсюда: \\\\\n",
        "$$\\xi_i := \\frac{\\partial ϕ}{∂\\theta_i} = \n",
        "\\begin{cases}\n",
        "  0, & y_j \\cdot φ_j(θ) > 1 \\\\\n",
        "  - y_j x^i_j, & y_j \\cdot φ_j(θ) < 1\n",
        "\\end{cases}$$\n",
        "\n",
        "(полагая $x^3_j$ = 1) \\\\\n",
        "\n",
        "Вычислим производные нормы $||\\theta||^2$:\n",
        "$$||\\theta||^2 = \\theta_1^2 + \\theta_2^2 + \\theta_3^2 $$\n",
        "\n",
        "$$\\frac{\\partial ||\\theta||^2}{∂θ_i} = 2\\theta_i$$\n",
        "$$\\nabla ||\\theta||^2 = \\begin{pmatrix} 2θ_1 \\\\ 2θ_2 \\\\ 2θ_3 \\end{pmatrix}$$\n",
        "\n",
        "Итого:\n",
        "$$\\frac{\\partial \\mathfrak{L}}{∂\\theta_i} = 0.2 \\theta_i + \\frac{1}{N} \\sum_{j=1}^N \\xi^i_j$$"
      ],
      "metadata": {
        "id": "esET7ZTb2tEd"
      },
      "id": "esET7ZTb2tEd"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ca79f1da",
      "metadata": {
        "id": "ca79f1da"
      },
      "outputs": [],
      "source": [
        "theta = theta0\n",
        "def gradient(X, y, theta):\n",
        "    der_theta = np.asarray(theta)\n",
        "    deltas = y * f(X, der_theta)\n",
        "    ext_der = (deltas < 1).astype(int) * (-y)\n",
        "    der_mean = np.zeros_like(der_theta)\n",
        "    der_mean[:2] = np.mean(ext_der[:, np.newaxis] * X, axis=0)\n",
        "    der_mean[2] = np.mean(ext_der)\n",
        "    return 0.1 * 2 * der_theta + der_mean\n",
        "\n",
        "for i in range(1000):\n",
        "    grad = gradient(X, y, theta)\n",
        "    theta -= lr * grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "011ca5b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "011ca5b7",
        "outputId": "c4292304-64c4-4b8a-eb21-4e5d88fd7832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized theta: [ 1.25218494  1.         -0.24781506]\n",
            "Prediction: [ 0.75218494  2.00436988  1.00436988 -0.37390753 -0.74781506]\n",
            "Loss: 0.48815643632236133\n"
          ]
        }
      ],
      "source": [
        "print(\"Optimized theta:\", theta0)\n",
        "print(\"Prediction:\", f(X, theta))\n",
        "print(\"Loss:\", loss(X, y, theta))\n",
        "\n",
        "with open(\"submission.yaml\", \"w\") as fp:\n",
        "    yaml.safe_dump({\"tasks\": [{\"task1\": {\"answer\": theta.tolist()}}]}, fp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}